{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10361187,"sourceType":"datasetVersion","datasetId":6416956}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# IMAGE CLASSIFIER\n\nWe are going to build an image classifier that when given a picture, can classify whether it is an image or a fish","metadata":{}},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\nimport torch.optim as optim\nimport torch.nn.functional as F","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T18:12:38.438827Z","iopub.execute_input":"2025-01-04T18:12:38.439236Z","iopub.status.idle":"2025-01-04T18:12:38.444186Z","shell.execute_reply.started":"2025-01-04T18:12:38.439200Z","shell.execute_reply":"2025-01-04T18:12:38.443064Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"## Dataset\n\nWe are going to use [imagenet](https://www.image-net.org/) a database with over hundred of thousands of images. It contains more than 14 million images and 20,000 image categories. Itâ€™s the standard that all image classifiers judge themselves against","metadata":{}},{"cell_type":"code","source":"import os\nfrom PIL import Image\nimport torchvision\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\n\n# Define a helper function to validate images\ndef is_valid_image(filepath):\n    try:\n        with Image.open(filepath) as img:\n            img.verify()  # Verify if it's a valid image\n        return True\n    except Exception:\n        return False\n\n# Custom ImageFolder to handle corrupted files\nclass SafeImageFolder(torchvision.datasets.ImageFolder):\n    def __init__(self, root, transform=None):\n        super().__init__(root, transform)\n        self.samples = [(path, label) for path, label in self.samples if is_valid_image(path)]\n\n# Data transforms\ndata_transforms = transforms.Compose([\n    transforms.Resize((64, 64)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n\n# Train Data\ntrain_data_path = \"/kaggle/input/fish-or-cat/images/train\"\ntrain_data = SafeImageFolder(root=train_data_path, transform=data_transforms)\n\n# Validation Data\nval_data_path = \"/kaggle/input/fish-or-cat/images/val\"\nval_data = SafeImageFolder(root=val_data_path, transform=data_transforms)\n\n# Test Data\ntest_data_path = \"/kaggle/input/fish-or-cat/images/test\"\ntest_data = SafeImageFolder(root=test_data_path, transform=data_transforms)\n\n# Dataloaders\nbatch_size = 64\ntrain_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=4)\nval_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, num_workers=4)\ntest_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=4)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T17:58:11.203731Z","iopub.execute_input":"2025-01-04T17:58:11.204164Z","iopub.status.idle":"2025-01-04T17:58:13.701056Z","shell.execute_reply.started":"2025-01-04T17:58:11.204131Z","shell.execute_reply":"2025-01-04T17:58:13.699930Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## Config","metadata":{}},{"cell_type":"code","source":"config = {\n    \"batch\" : 64,\n    \"epochs\": 50,\n    \"lr\":0.001\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T17:58:13.703155Z","iopub.execute_input":"2025-01-04T17:58:13.703492Z","iopub.status.idle":"2025-01-04T17:58:13.707669Z","shell.execute_reply.started":"2025-01-04T17:58:13.703462Z","shell.execute_reply":"2025-01-04T17:58:13.706553Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## DataLoader","metadata":{}},{"cell_type":"code","source":"train_dataloader = DataLoader(train_data, batch_size = config[\"batch\"])\nval_dataloader = DataLoader(val_data, batch_size = config[\"batch\"])\ntest_dataloader = DataLoader(test_data, batch_size = config[\"batch\"])\n\nfor image, label in train_dataloader:\n    print(f\"{image.shape}, {label.shape}\")\n    # the image shape should be (batch_size, channels, height, width) - so the first layer of the network should be channel X height X width\n    # the label should be (64)\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T17:58:13.708928Z","iopub.execute_input":"2025-01-04T17:58:13.709316Z","iopub.status.idle":"2025-01-04T17:58:14.036611Z","shell.execute_reply.started":"2025-01-04T17:58:13.709285Z","shell.execute_reply":"2025-01-04T17:58:14.035435Z"}},"outputs":[{"name":"stdout","text":"torch.Size([64, 3, 64, 64]), torch.Size([64])\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## Model Architecture","metadata":{}},{"cell_type":"code","source":"class CatorFish(nn.Module):\n    def __init__(self):\n        super(CatorFish, self).__init__()\n        self.fc1 = nn.Linear(12288, 84)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(84, 50)\n        self.relu2 = nn.ReLU()\n        self.fc3 = nn.Linear(50, 2)\n        # self.softmax = nn.Softmax()\n\n    def forward(self, x):\n        # x = x.view(-1, 12288) # flattening the image\n        x = x.view(x.size(0), -1)  # Flatten the input\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.relu2(x)\n        x = self.fc3(x)\n        # x = self.softmax(x)\n        return x\n\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device(\"cpu\")\nmodel = CatorFish()\nmodel.to(device)\nmodel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T18:10:17.660236Z","iopub.execute_input":"2025-01-04T18:10:17.660623Z","iopub.status.idle":"2025-01-04T18:10:17.681312Z","shell.execute_reply.started":"2025-01-04T18:10:17.660587Z","shell.execute_reply":"2025-01-04T18:10:17.680035Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"CatorFish(\n  (fc1): Linear(in_features=12288, out_features=84, bias=True)\n  (relu): ReLU()\n  (fc2): Linear(in_features=84, out_features=50, bias=True)\n  (relu2): ReLU()\n  (fc3): Linear(in_features=50, out_features=2, bias=True)\n)"},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"## OPTIMIZER & LOSS FUNCTIONS","metadata":{}},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer  = optim.Adam(model.parameters(), lr = config[\"lr\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T18:10:52.218286Z","iopub.execute_input":"2025-01-04T18:10:52.218698Z","iopub.status.idle":"2025-01-04T18:10:52.223932Z","shell.execute_reply.started":"2025-01-04T18:10:52.218661Z","shell.execute_reply":"2025-01-04T18:10:52.222620Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"## TRAIN LOOP","metadata":{}},{"cell_type":"code","source":"def train(model, train_loader, val_loader, criterion, optimizer, epochs, device):\n    for epoch in range(epochs):\n        training_loss = 0.0\n        valid_loss = 0.0\n        model.train()\n        for images, labels in train_loader:\n            # Forward pass\n            images.to(device)\n            labels.to(device)\n            predictions = model(images)\n            loss = criterion(predictions, labels)\n    \n            # backward pass\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            training_loss += loss.data.item()\n        training_loss /= len(train_loader)\n\n        model.eval()\n        num_correct = 0\n        num_examples = 0\n        for batch in val_loader:\n            inputs, targets = batch\n            inputs = inputs.to(device)\n            output = model(inputs)\n            targets = targets.to(device)\n            loss = criterion(output,targets)\n            valid_loss += loss.data.item()\n            correct = torch.eq(torch.max(F.softmax(output), dim=1)[1],targets).view(-1)\n            num_correct += torch.sum(correct).item()\n            num_examples += correct.shape[0]\n        valid_loss /= len(val_loader)\n    print('Epoch: {}, Training Loss: {:.2f},Validation Loss: {:.2f},accuracy = {:.2f}'.format(epoch, training_loss,valid_loss, num_correct / num_examples))    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T18:14:17.374744Z","iopub.execute_input":"2025-01-04T18:14:17.375163Z","iopub.status.idle":"2025-01-04T18:14:17.383682Z","shell.execute_reply.started":"2025-01-04T18:14:17.375124Z","shell.execute_reply":"2025-01-04T18:14:17.382305Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"train(model, train_dataloader, test_dataloader, criterion, optimizer, config[\"epochs\"], device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T18:14:22.829196Z","iopub.execute_input":"2025-01-04T18:14:22.829538Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-27-ff298e421fc0>:30: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  correct = torch.eq(torch.max(F.softmax(output), dim=1)[1],targets).view(-1)\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}