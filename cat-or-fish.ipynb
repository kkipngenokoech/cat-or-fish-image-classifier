{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10361187,"sourceType":"datasetVersion","datasetId":6416956}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# IMAGE CLASSIFIER\n\nWe are going to build an image classifier that when given a picture, can classify whether it is an image or a fish","metadata":{}},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\nimport torch.optim as optim\nimport torch.nn.functional as F","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T08:17:22.391124Z","iopub.execute_input":"2025-01-05T08:17:22.391499Z","iopub.status.idle":"2025-01-05T08:17:27.686766Z","shell.execute_reply.started":"2025-01-05T08:17:22.391471Z","shell.execute_reply":"2025-01-05T08:17:27.685637Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"## Dataset\n\nWe are going to use [imagenet](https://www.image-net.org/) a database with over hundred of thousands of images. It contains more than 14 million images and 20,000 image categories. Itâ€™s the standard that all image classifiers judge themselves against","metadata":{}},{"cell_type":"code","source":"import os\nfrom PIL import Image\nimport torchvision\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\n\n# Define a helper function to validate images\ndef is_valid_image(filepath):\n    try:\n        with Image.open(filepath) as img:\n            img.verify()  # Verify if it's a valid image\n        return True\n    except Exception:\n        return False\n\n# Custom ImageFolder to handle corrupted files\nclass SafeImageFolder(torchvision.datasets.ImageFolder):\n    def __init__(self, root, transform=None):\n        super().__init__(root, transform)\n        self.samples = [(path, label) for path, label in self.samples if is_valid_image(path)]\n\n# Data transforms\ndata_transforms = transforms.Compose([\n    transforms.Resize((64, 64)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n\n# Train Data\ntrain_data_path = \"/kaggle/input/fish-or-cat/images/train\"\ntrain_data = SafeImageFolder(root=train_data_path, transform=data_transforms)\n\n# Validation Data\nval_data_path = \"/kaggle/input/fish-or-cat/images/val\"\nval_data = SafeImageFolder(root=val_data_path, transform=data_transforms)\n\n# Test Data\ntest_data_path = \"/kaggle/input/fish-or-cat/images/test\"\ntest_data = SafeImageFolder(root=test_data_path, transform=data_transforms)\n\n# Dataloaders\nbatch_size = 64\ntrain_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=4)\nval_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, num_workers=4)\ntest_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=4)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T08:17:27.687850Z","iopub.execute_input":"2025-01-05T08:17:27.688405Z","iopub.status.idle":"2025-01-05T08:17:37.844167Z","shell.execute_reply.started":"2025-01-05T08:17:27.688369Z","shell.execute_reply":"2025-01-05T08:17:37.842646Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## Config","metadata":{}},{"cell_type":"code","source":"config = {\n    \"batch\" : 64,\n    \"epochs\": 50,\n    \"lr\":0.001\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T08:17:37.845515Z","iopub.execute_input":"2025-01-05T08:17:37.845977Z","iopub.status.idle":"2025-01-05T08:17:37.851325Z","shell.execute_reply.started":"2025-01-05T08:17:37.845916Z","shell.execute_reply":"2025-01-05T08:17:37.849974Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## DataLoader","metadata":{}},{"cell_type":"code","source":"train_dataloader = DataLoader(train_data, batch_size = config[\"batch\"])\nval_dataloader = DataLoader(val_data, batch_size = config[\"batch\"])\ntest_dataloader = DataLoader(test_data, batch_size = config[\"batch\"])\n\nfor image, label in train_dataloader:\n    print(f\"{image.shape}, {label.shape}\")\n    # the image shape should be (batch_size, channels, height, width) - so the first layer of the network should be channel X height X width\n    # the label should be (64)\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T08:17:37.852434Z","iopub.execute_input":"2025-01-05T08:17:37.852752Z","iopub.status.idle":"2025-01-05T08:17:38.295732Z","shell.execute_reply.started":"2025-01-05T08:17:37.852724Z","shell.execute_reply":"2025-01-05T08:17:38.294153Z"}},"outputs":[{"name":"stdout","text":"torch.Size([64, 3, 64, 64]), torch.Size([64])\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## Model Architecture","metadata":{}},{"cell_type":"code","source":"class CatorFish(nn.Module):\n    def __init__(self):\n        super(CatorFish, self).__init__()\n        self.fc1 = nn.Linear(12288, 84)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(84, 50)\n        self.relu2 = nn.ReLU()\n        self.fc3 = nn.Linear(50, 2)\n        # self.softmax = nn.Softmax()\n\n    def forward(self, x):\n        # x = x.view(-1, 12288) # flattening the image\n        x = x.view(x.size(0), -1)  # Flatten the input\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.relu2(x)\n        x = self.fc3(x)\n        # x = self.softmax(x)\n        return x\n\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device(\"cpu\")\nmodel = CatorFish()\nmodel.to(device)\nmodel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T08:17:38.298598Z","iopub.execute_input":"2025-01-05T08:17:38.299087Z","iopub.status.idle":"2025-01-05T08:17:38.332543Z","shell.execute_reply.started":"2025-01-05T08:17:38.299046Z","shell.execute_reply":"2025-01-05T08:17:38.331339Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"CatorFish(\n  (fc1): Linear(in_features=12288, out_features=84, bias=True)\n  (relu): ReLU()\n  (fc2): Linear(in_features=84, out_features=50, bias=True)\n  (relu2): ReLU()\n  (fc3): Linear(in_features=50, out_features=2, bias=True)\n)"},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"## OPTIMIZER & LOSS FUNCTIONS","metadata":{}},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer  = optim.Adam(model.parameters(), lr = config[\"lr\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T08:17:38.333911Z","iopub.execute_input":"2025-01-05T08:17:38.334198Z","iopub.status.idle":"2025-01-05T08:17:38.339654Z","shell.execute_reply.started":"2025-01-05T08:17:38.334175Z","shell.execute_reply":"2025-01-05T08:17:38.338617Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## TRAIN LOOP","metadata":{}},{"cell_type":"code","source":"def train(model, train_loader, val_loader, criterion, optimizer, epochs, device):\n    for epoch in range(epochs):\n        training_loss = 0.0\n        valid_loss = 0.0\n        model.train()\n        for images, labels in train_loader:\n            # Forward pass\n            images.to(device)\n            labels.to(device)\n            predictions = model(images)\n            loss = criterion(predictions, labels)\n    \n            # backward pass\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            training_loss += loss.data.item()\n        training_loss /= len(train_loader)\n\n        model.eval()\n        num_correct = 0\n        num_examples = 0\n        for batch in val_loader:\n            inputs, targets = batch\n            inputs = inputs.to(device)\n            output = model(inputs)\n            targets = targets.to(device)\n            loss = criterion(output,targets)\n            valid_loss += loss.data.item()\n            correct = torch.eq(torch.max(F.softmax(output), dim=1)[1],targets).view(-1)\n            num_correct += torch.sum(correct).item()\n            num_examples += correct.shape[0]\n        valid_loss /= len(val_loader)\n        print('Epoch: {}, Training Loss: {:.2f},Validation Loss: {:.2f},accuracy = {:.2f}'.format(epoch, training_loss,valid_loss, num_correct / num_examples))    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T08:17:38.340916Z","iopub.execute_input":"2025-01-05T08:17:38.341321Z","iopub.status.idle":"2025-01-05T08:17:38.359767Z","shell.execute_reply.started":"2025-01-05T08:17:38.341281Z","shell.execute_reply":"2025-01-05T08:17:38.358539Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"train(model, train_dataloader, test_dataloader, criterion, optimizer, config[\"epochs\"], device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T08:17:38.360810Z","iopub.execute_input":"2025-01-05T08:17:38.361182Z","iopub.status.idle":"2025-01-05T08:20:59.493010Z","shell.execute_reply.started":"2025-01-05T08:17:38.361141Z","shell.execute_reply":"2025-01-05T08:20:59.491813Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-7-cca5c1ac2298>:30: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  correct = torch.eq(torch.max(F.softmax(output), dim=1)[1],targets).view(-1)\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 0, Training Loss: 1.61,Validation Loss: 1.63,accuracy = 0.45\nEpoch: 1, Training Loss: 1.16,Validation Loss: 0.73,accuracy = 0.49\nEpoch: 2, Training Loss: 0.66,Validation Loss: 0.58,accuracy = 0.72\nEpoch: 3, Training Loss: 0.45,Validation Loss: 0.64,accuracy = 0.70\nEpoch: 4, Training Loss: 0.50,Validation Loss: 0.56,accuracy = 0.77\nEpoch: 5, Training Loss: 0.34,Validation Loss: 0.64,accuracy = 0.74\nEpoch: 6, Training Loss: 0.39,Validation Loss: 0.58,accuracy = 0.77\nEpoch: 7, Training Loss: 0.29,Validation Loss: 0.67,accuracy = 0.74\nEpoch: 8, Training Loss: 0.33,Validation Loss: 0.61,accuracy = 0.80\nEpoch: 9, Training Loss: 0.25,Validation Loss: 0.66,accuracy = 0.75\nEpoch: 10, Training Loss: 0.27,Validation Loss: 0.65,accuracy = 0.80\nEpoch: 11, Training Loss: 0.20,Validation Loss: 0.77,accuracy = 0.77\nEpoch: 12, Training Loss: 0.24,Validation Loss: 0.70,accuracy = 0.79\nEpoch: 13, Training Loss: 0.16,Validation Loss: 0.82,accuracy = 0.76\nEpoch: 14, Training Loss: 0.19,Validation Loss: 0.79,accuracy = 0.78\nEpoch: 15, Training Loss: 0.15,Validation Loss: 0.80,accuracy = 0.79\nEpoch: 16, Training Loss: 0.14,Validation Loss: 0.84,accuracy = 0.78\nEpoch: 17, Training Loss: 0.12,Validation Loss: 0.85,accuracy = 0.78\nEpoch: 18, Training Loss: 0.10,Validation Loss: 0.91,accuracy = 0.77\nEpoch: 19, Training Loss: 0.10,Validation Loss: 0.94,accuracy = 0.76\nEpoch: 20, Training Loss: 0.09,Validation Loss: 0.97,accuracy = 0.78\nEpoch: 21, Training Loss: 0.07,Validation Loss: 1.05,accuracy = 0.76\nEpoch: 22, Training Loss: 0.08,Validation Loss: 1.04,accuracy = 0.79\nEpoch: 23, Training Loss: 0.05,Validation Loss: 1.13,accuracy = 0.76\nEpoch: 24, Training Loss: 0.05,Validation Loss: 1.20,accuracy = 0.74\nEpoch: 25, Training Loss: 0.06,Validation Loss: 1.15,accuracy = 0.76\nEpoch: 26, Training Loss: 0.04,Validation Loss: 1.24,accuracy = 0.75\nEpoch: 27, Training Loss: 0.04,Validation Loss: 1.24,accuracy = 0.74\nEpoch: 28, Training Loss: 0.03,Validation Loss: 1.28,accuracy = 0.75\nEpoch: 29, Training Loss: 0.03,Validation Loss: 1.35,accuracy = 0.75\nEpoch: 30, Training Loss: 0.02,Validation Loss: 1.37,accuracy = 0.75\nEpoch: 31, Training Loss: 0.02,Validation Loss: 1.41,accuracy = 0.77\nEpoch: 32, Training Loss: 0.02,Validation Loss: 1.43,accuracy = 0.75\nEpoch: 33, Training Loss: 0.01,Validation Loss: 1.47,accuracy = 0.75\nEpoch: 34, Training Loss: 0.01,Validation Loss: 1.49,accuracy = 0.74\nEpoch: 35, Training Loss: 0.01,Validation Loss: 1.55,accuracy = 0.74\nEpoch: 36, Training Loss: 0.01,Validation Loss: 1.56,accuracy = 0.76\nEpoch: 37, Training Loss: 0.01,Validation Loss: 1.58,accuracy = 0.76\nEpoch: 38, Training Loss: 0.00,Validation Loss: 1.62,accuracy = 0.76\nEpoch: 39, Training Loss: 0.00,Validation Loss: 1.64,accuracy = 0.76\nEpoch: 40, Training Loss: 0.00,Validation Loss: 1.65,accuracy = 0.76\nEpoch: 41, Training Loss: 0.00,Validation Loss: 1.68,accuracy = 0.76\nEpoch: 42, Training Loss: 0.00,Validation Loss: 1.71,accuracy = 0.76\nEpoch: 43, Training Loss: 0.00,Validation Loss: 1.74,accuracy = 0.76\nEpoch: 44, Training Loss: 0.00,Validation Loss: 1.75,accuracy = 0.76\nEpoch: 45, Training Loss: 0.00,Validation Loss: 1.77,accuracy = 0.75\nEpoch: 46, Training Loss: 0.00,Validation Loss: 1.79,accuracy = 0.76\nEpoch: 47, Training Loss: 0.00,Validation Loss: 1.81,accuracy = 0.76\nEpoch: 48, Training Loss: 0.00,Validation Loss: 1.82,accuracy = 0.76\nEpoch: 49, Training Loss: 0.00,Validation Loss: 1.83,accuracy = 0.76\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"The model is experiencing overfitting, because the training loss flattens, whereas the validation loss is still high. \n\n- Training loss approaches zero, indicating that the model is learning the training data extremely well.\n- Validation loss steadily increases, suggesting that the model is failing to generalize to unseen data.","metadata":{}},{"cell_type":"markdown","source":"## Save the Model","metadata":{}},{"cell_type":"code","source":"torch.save(model, \"catorfish.pth\")\n# torch.save(model.parameters(), \"catorfishparams.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T08:38:52.034338Z","iopub.execute_input":"2025-01-05T08:38:52.034687Z","iopub.status.idle":"2025-01-05T08:38:52.046644Z","shell.execute_reply.started":"2025-01-05T08:38:52.034660Z","shell.execute_reply":"2025-01-05T08:38:52.045348Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}